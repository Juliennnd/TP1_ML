# -*- coding: utf-8 -*-
"""TP1_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B3E9xPuG9VstWo1B7ma33r1b5eqPB9gT
"""

import pandas as pd
import io
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx

from keras.utils import to_categorical
from google.colab import files
from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.preprocessing import MinMaxScaler

from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc

files.upload ()

data = pd.read_csv("stress_classification.csv", sep=';')

data.head()

data.isnull().sum()

# Plot histograms and density plots for each feature
for feature in data.columns:
    plt.figure()
    sns.histplot(data=data, x=feature, kde=True)
    plt.title(feature)

plt.show()

sns.pairplot(data, hue='stress_level')

# Create density plots for each feature
for feature in data.columns:
    sns.kdeplot(data=data[feature])
    plt.title(f'Density plot of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Density')
    plt.show()

# Calculate the correlation matrix
corr_matrix = data.corr()

# Plot a heatmap of the correlations
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')

#other type of correlation vizualisation
# Create a scatter plot matrix
pd.plotting.scatter_matrix(data, alpha=0.2, figsize=(10,10), diagonal='hist')
plt.show()

#Other visualization 
# Calculate the correlation matrix
corr_matrix = data.corr()

# Create a graph from the correlation matrix
G = nx.from_pandas_adjacency(corr_matrix)

# Set the node labels to the feature names
node_labels = dict(zip(range(len(data.columns)), data.columns))
G = nx.relabel_nodes(G, node_labels)

# Create the correlation network plot
pos = nx.spring_layout(G)
nx.draw_networkx_nodes(G, pos, node_size=500, node_color='lightblue')
nx.draw_networkx_edges(G, pos, edge_color='gray')
nx.draw_networkx_labels(G, pos, font_size=12, font_family='sans-serif')
plt.axis('off')
plt.show()

# Split the data into training and testing datasets
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)


print(f'Training dataset size: {len(train_data)}')
print(f'Testing dataset size: {len(test_data)}')

# Separate the target variable from the features
X = data.drop('stress_level', axis=1)
y = data['stress_level']

# Scale the features using min-max scaling
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and validation datasets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)


# Try different values of k and evaluate the performance using cross-validation
k_values = range(1, 21)
cv_scores = []
for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=5)
    cv_scores.append(scores.mean())

# Choose the best value of k based on the validation scores
best_k = k_values[cv_scores.index(max(cv_scores))]
print(f'Best value of k: {best_k}')

# Fit the model on the training dataset using the best value of k
knn = KNeighborsClassifier(n_neighbors=best_k)
knn.fit(X_train, y_train)

# Predict the probabilities for each class using the test dataset
y_pred_prob = knn.predict_proba(X_test)

# Get the predicted labels by finding the class with the highest probability
y_pred = np.argmax(y_pred_prob, axis=1)

# Calculate the various metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')
roc_auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovo')

print(f'Accuracy: {accuracy:.3f}')
print(f'Precision: {precision:.3f}')
print(f'Recall: {recall:.3f}')
print(f'F1-score: {f1:.3f}')
print(f'ROC-AUC score: {roc_auc:.3f}')

# Get the predicted labels
y_pred = knn.predict(X_test)

# Create the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix as a heatmap
sns.heatmap(cm, annot=True, cmap='Blues')

# Compute ROC curve and ROC area for each class
fpr = {}
tpr = {}
roc_auc = {}
n_classes = len(np.unique(y))

y_test_onehot = to_categorical(y_test)
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_onehot[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test_onehot.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curves
plt.figure(figsize=(8, 6))
lw = 2
plt.plot(fpr["micro"], tpr["micro"], color='darkorange',
         lw=lw, label='micro-average ROC curve (area = %0.2f)' % roc_auc["micro"])
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], lw=lw,
             label='ROC curve of class %d (area = %0.2f)' % (i, roc_auc[i]))

plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.show()

# Test different value of K and plot its metrics 
 
k_values = range(1, 300)

# Initialize lists to store the performance metrics for each K value
accuracy_scores = []
precision_scores = []
recall_scores = []
f1_scores = []
roc_auc_scores = []

# Loop through each K value and calculate the performance metrics using cross-validation
for k in k_values:
    # Initialize a KNN model with the current K value
    knn = KNeighborsClassifier(n_neighbors=k)
    
    # Fit the model and make predictions on the test set
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    y_pred_prob = knn.predict_proba(X_test)
    
    # Calculate the different performance metrics and store them in the lists
    accuracy_scores.append(accuracy_score(y_test, y_pred))
    precision_scores.append(precision_score(y_test, y_pred, average='macro'))
    recall_scores.append(recall_score(y_test, y_pred, average='macro'))
    f1_scores.append(f1_score(y_test, y_pred, average='macro'))
    roc_auc_scores.append(roc_auc_score(y_test, y_pred_prob, multi_class='ovo'))
    
# Plot the different performance metrics against the K values
plt.plot(k_values, accuracy_scores, label='Accuracy')
plt.plot(k_values, precision_scores, label='Precision')
plt.plot(k_values, recall_scores, label='Recall')
plt.plot(k_values, f1_scores, label='F1-score')
plt.plot(k_values, roc_auc_scores, label='ROC-AUC score')
plt.xlabel('K')
plt.ylabel('Score')
plt.legend()
plt.show()

# DECISION TREE

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Try different values of max_depth and evaluate the performance using cross-validation
depth_values = range(1, 21)
f1_scores = []

for depth in depth_values:
    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)
    scores = cross_val_score(tree, X_train, y_train, cv=5, scoring='f1_macro')
    f1_scores.append(scores.mean())

# Plot the F1-score for each value of max_depth
plt.plot(depth_values, f1_scores)
plt.xlabel('Max depth')
plt.ylabel('F1-score')
plt.show()

# Choose the value of max_depth that gives the best performance on the validation set
best_depth = depth_values[np.argmax(f1_scores)]
print(f'Best max_depth: {best_depth}')

# Fit the model on the training dataset
tree = DecisionTreeClassifier(max_depth=best_depth, random_state=42)
tree.fit(X_train, y_train)

# Evaluate the performance on the validation dataset
accuracy_tree = accuracy_score(y_val, y_val_pred)
precision_tree = precision_score(y_val, y_val_pred, average='macro')
recall_tree = recall_score(y_val, y_val_pred, average='macro')
f1_tree = f1_score(y_val, y_val_pred, average='macro')
roc_auc_tree = roc_auc_score(y_val, tree.predict_proba(X_val), multi_class='ovo')



print(f'Accuracy: {accuracy:.3f}')
print(f'Precision: {precision:.3f}')
print(f'Recall: {recall:.3f}')
print(f'F1-score: {f1:.3f}')
print(f'ROC-AUC score: {roc_auc:.3f}')

"""**We can observe that all the metrics are better with the KNN algorithm for this dataset in particular as the graphs below can show **"""

k_values=1
max_depths = 4
# Define the evaluation metrics
metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC']
knn_scores = [accuracy, precision, recall, f1, roc_auc]
tree_scores = [accuracy_tree, precision_tree, recall_tree, f1_tree, roc_auc_tree]
#print (tree_scores)
#print (knn_scores)

# Plotting accuracy, precision, recall, f1-score and ROC AUC
fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))
axs = axs.flatten()

# KNN Model
axs[0].plot(k_values, accuracy, '-o', label='Accuracy')
axs[0].plot(k_values, precision, '-o', label='Precision')
axs[0].plot(k_values, recall, '-o', label='Recall')
axs[0].plot(k_values, f1, '-o', label='F1-score')
axs[0].plot(k_values, roc_auc, '-o', label='ROC AUC')
axs[0].set_xlabel('k')
axs[0].set_ylabel('Score')
axs[0].set_title('KNN Model')
axs[0].legend()
axs[0].set_ylim([0.970, 1.01])

# Tree Model
axs[1].plot(max_depths, accuracy_tree, '-o', label='Accuracy')
axs[1].plot(max_depths, precision_tree, '-o', label='Precision')
axs[1].plot(max_depths, recall_tree, '-o', label='Recall')
axs[1].plot(max_depths, f1_tree, '-o', label='F1-score')
axs[1].plot(max_depths, roc_auc_tree, '-o', label='ROC AUC')
axs[1].set_xlabel('Max Depth')
axs[1].set_ylabel('Score')
axs[1].set_title('Tree Model')
axs[1].legend()
axs[1].set_ylim([0.970, 1.01])

plt.show()

#TO GO FURTHER

from tabulate import tabulate

#load the data (redoing everything to just run 1st cell and this one)
#files.upload ()
#data = pd.read_csv("stress_classification.csv", sep=';')

# split data into features and target
X = data.drop(['stress_level'], axis=1)
y = data['stress_level']

# Scale the features using min-max scaling
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# split data into train and test sets (20% of test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# initialize the models
knn = KNeighborsClassifier(n_neighbors=1)
tree = DecisionTreeClassifier(max_depth=4)
logistic = LogisticRegression()
forest = RandomForestClassifier(n_estimators=100, max_depth=4)

# train the models
knn.fit(X_train, y_train)
tree.fit(X_train, y_train)
logistic.fit(X_train, y_train)
forest.fit(X_train, y_train)

# predict on the test set
knn_y_pred = knn.predict(X_test)
tree_y_pred = tree.predict(X_test)
logistic_y_pred = logistic.predict(X_test)
forest_y_pred = forest.predict(X_test)


# calculate evaluation metrics for each model
knn_accuracy = accuracy_score(y_test, knn_y_pred)
tree_accuracy = accuracy_score(y_test, tree_y_pred)
logistic_accuracy = accuracy_score(y_test, logistic_y_pred)
forest_accuracy = accuracy_score(y_test, forest_y_pred)

knn_precision = precision_score(y_test, knn_y_pred, average='weighted')
tree_precision = precision_score(y_test, tree_y_pred, average='weighted')
logistic_precision = precision_score(y_test, logistic_y_pred, average='weighted')
forest_precision = precision_score(y_test, forest_y_pred, average='weighted')

knn_recall = recall_score(y_test, knn_y_pred, average='weighted')
tree_recall = recall_score(y_test, tree_y_pred, average='weighted')
logistic_recall = recall_score(y_test, logistic_y_pred, average='weighted')
forest_recall = recall_score(y_test, forest_y_pred, average='weighted')

knn_f1 = f1_score(y_test, knn_y_pred, average='weighted')
tree_f1 = f1_score(y_test, tree_y_pred, average='weighted')
logistic_f1 = f1_score(y_test, logistic_y_pred, average='weighted')
forest_f1 = f1_score(y_test, forest_y_pred, average='weighted')

#knn_roc_auc = roc_auc_score(y_test, knn_y_pred, multi_class='ovr', average='weighted')
#tree_roc_auc = roc_auc_score(y_test, tree_y_pred, multi_class='ovr', average='weighted')
#logistic_roc_auc = roc_auc_score(y_test, logistic_y_pred, multi_class='ovr', average='weighted')
#forest_roc_auc = roc_auc_score(y_test, forest_y_pred, multi_class='ovr', average='weighted')

# plot results
models = ['KNN', 'Decision Tree', 'Logistic Regression', 'Random Forest']
accuracy = [knn_accuracy, tree_accuracy, logistic_accuracy, forest_accuracy]
precision = [knn_precision, tree_precision, logistic_precision, forest_precision]
recall = [knn_recall, tree_recall, logistic_recall, forest_recall]
f1 = [knn_f1, tree_f1, logistic_f1, forest_f1]
#roc_auc = [knn_roc_auc, tree_roc_auc, logistic_roc_auc, forest_roc_auc]

#print to see each value
headers = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score']
table = zip(models, accuracy, precision, recall, f1)
print(tabulate(table, headers=headers))

fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15,10))
axs = axs.flatten()

# plot accuracy
axs[0].bar(models, accuracy)
axs[0].set_title('Accuracy')
axs[0].set_ylim([0.9, 1.01])

# plot precision
axs[1].bar(models, precision)
axs[1].set_title('Precision')
axs[1].set_ylim([0.9, 1.01])

# plot recall
axs[2].bar(models, recall)
axs[2].set_title('Recall')
axs[2].set_ylim([0.9, 1.01])

# plot f1-score
axs[3].bar(models, f1)
axs[3].set_title('F1-score')
axs[3].set_ylim([0.9, 1.01])

# plot roc-auc
#axs[4].bar(models, roc_auc)
#axs[4].set_title('ROC AUC')
#axs[4].set_ylim([0.6, 1.01])

plt.tight_layout()
plt.show()